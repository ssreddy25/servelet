<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>The ultimate CI/CD resource guide</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/10/20/ultimate-cicd-resource-guide" /><author><name>Heiker Medina</name></author><id>32737cb1-c7f6-483b-93bb-6729cb90017c</id><updated>2022-10-20T07:00:00Z</updated><published>2022-10-20T07:00:00Z</published><summary type="html">&lt;p&gt;You've found the right place for content on &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous integration/continuous deployment (CI/CD)&lt;/a&gt;. We've gathered our highest-performing articles from the past year on this topic on Red Hat Developer. This article will introduce you to all things related to CI/CD.&lt;/p&gt; &lt;h2&gt;What is CI/CD and why is it important for developers?&lt;/h2&gt; &lt;p&gt;Continuous integration (CI) and continuous deployment (CD) are development processes making use of automated tools to produce high-quality software.&lt;/p&gt; &lt;p&gt;CI ensures that any code submitted by each developer works together with all other code in the project. Typically, CI works by running regression tests.&lt;/p&gt; &lt;p&gt;CD involves further automation to make sure that the latest accepted versions of a project enter production, and that all the pieces deployed together are compatible.&lt;/p&gt; &lt;p&gt;Numerous tools, such as integrated development environments and version control systems, help you build software. But when it comes to creating software that customers trust—and even love—you need to pay attention to the details. A good CI/CD environment ensures that testing, integration, and deployment are fast, easy, and accurate. CI/CD allows you to iterate faster, build more reliable code, and deliver better customer experiences.&lt;/p&gt; &lt;h2&gt;Recent articles to explore about CI/CD&lt;/h2&gt; &lt;p&gt;Here are eight great Red Hat Developer articles on this topic:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2021/05/24/deploy-helm-charts-jenkins-cicd-red-hat-openshift-4"&gt;Deploy Helm charts with Jenkins CI/CD&lt;/a&gt;:&lt;/strong&gt; Learn how to use Jenkins's CI/CD capabilities to deploy a Helm chart using a &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; 4 cluster. Helm is a package manager for Kubernetes that uses a packaging format called &lt;em&gt;charts&lt;/em&gt;. These charts have all of the Kubernetes resources required to deploy an application, such as deployments and services.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/13/developers-guide-cicd-and-gitops-jenkins-pipelines"&gt;A developer's guide to CI/CD and GitOps with Jenkins Pipelines&lt;/a&gt;:&lt;/strong&gt; &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; and CI/CD have a lot to offer each other. This article guides you through the use of a Jenkinsfile to create deployments that combine CI/CD and GitOps.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/08/01/containerize-net-applications-without-writing-dockerfiles"&gt;Containerize .NET applications without writing Dockerfiles&lt;/a&gt;:&lt;/strong&gt; Discover how to use a tool named dotnet build-image to create Dockerfiles and containerized images from &lt;a href="https://developers.redhat.com/topics/dotnet"&gt;.NET&lt;/a&gt; applications. You will also learn how to use this tool in a GitHub workflow to create an image from a .NET application and push it to a repository.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/05/automate-cicd-pull-requests-argo-cd-applicationsets#"&gt;Automate CI/CD on pull requests with Argo CD ApplicationSets&lt;/a&gt;:&lt;/strong&gt; The quest to further automate building, testing, and deployment is inspiring new features in Argo CD, &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, and other tools. This article shows how to improve feature testing by automating builds and the creation of Kubernetes environments. ApplicationSets in Argo CD, together with Tekton, create a CI/CD system that includes feature branch testing on OpenShift.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2021/06/01/how-create-better-front-end-developer-experience"&gt;How to create a better front-end developer experience&lt;/a&gt;:&lt;/strong&gt; Explore common pain points that can complicate the development process, and learn how to address them to foster better developer experiences. Take developing a form using React, for example. If developers can develop the form without difficulty, it will likely be a positive experience for the customer as well.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/02/09/gitops-using-red-hat-openshift-console-49-and-410#"&gt;OpenShift support for GitOps processes&lt;/a&gt;:&lt;/strong&gt; This article explains how &lt;a href="https://next.redhat.com/project/gitops-primer/"&gt;OpenShift's GitOps Operator&lt;/a&gt; works and highlights improvements in OpenShift 4.9 and 4.10. The article includes a video showing how to use the developer console to manage cluster configurations and deploy cloud-native applications using OpenShift. Follow up this article by trying out a &lt;a href="https://developers.redhat.com/learn/openshift/develop-gitops"&gt;learning path about GitOps&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/20/integrate-iso-20022-payments-messaging-cicd"&gt;Integrate ISO 20022 payments messaging with CI/CD&lt;/a&gt;:&lt;/strong&gt; An increasing number of financial institutions are embracing ISO 20022 standards for payments and securities transactions. This article shows how to use the message conversion platform from Trace Financial (a Red Hat ISV) and Red Hat Fuse to convert messages between the SWIFT MT and MX formats for financial data. Key benefits of the MX message set include the ability to capture richer data, flexibility, and a machine-readable format.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2021/10/12/tools-and-practices-remote-development-teams"&gt;Tools and practices for remote development teams&lt;/a&gt;:&lt;/strong&gt; This article explores a few tools and practices that can help you work from home. Ideas include shared IDEs to facilitate collaboration, a repository of self-service and single-click workspaces for IDE consistency, and triggering CI/CD pipelines from source control systems to automate manual operations tasks.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;The Developer Sandbox is a great place to start&lt;/h2&gt; &lt;p&gt;Interested in developing your first application at no cost? Test out the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox on Red Hat OpenShift&lt;/a&gt; and learn by doing.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/10/20/ultimate-cicd-resource-guide" title="The ultimate CI/CD resource guide"&gt;The ultimate CI/CD resource guide&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Heiker Medina</dc:creator><dc:date>2022-10-20T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - 20 October 2022</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2022-10-20.html" /><category term="kogito" /><category term="drools" /><category term="quarkus" /><category term="resteasy" /><category term="byteman" /><category term="wildfly" /><author><name>Jason Porter</name><uri>https://www.jboss.org/people/jason-porter</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2022-10-20.html</id><updated>2022-10-20T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="kogito, drools, quarkus, resteasy, byteman, wildfly"&gt; &lt;h1&gt;This Week in JBoss - 20 October 2022&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Welcome back everyone! We’re here with another JBoss Editorial, highlighting some of the great work done in the community and within Red Hat as it relates to middleware. Like always, we have some news from the blogosphere and releases. No new videos out this week. Let’s get started!&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_releases"&gt;Releases&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/10/kogito-1-29-0-released.html"&gt;Kogito 1.29.0.Final&lt;/a&gt; - Mostly a bug fix release, but there are some breaking changes, please review before upgrading&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/10/drools-8-final-toward-a-modular-and-cloud-native-rule-engine.html"&gt;Drools 8&lt;/a&gt; - A brand new major release for Drools! This is great news for everyone. New features abound, and migrations as well. Be sure to check it out and read about everything that is new.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-13-2-final-released/"&gt;Quarkus 2.13.2.Final&lt;/a&gt; - Bug fixes and documentation improvements abound in this release. If you’re on 2.13, be sure to upgrade!&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://resteasy.dev/2022/10/10/resteasy-6.2.1.Final-release/"&gt;RESTEasy 6.2.1.Final&lt;/a&gt; - Some bug fixes, mostly component upgrades&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://bytemanblog.blogspot.com/2022/10/byteman-4020-has-been-released.html"&gt;Byteman 4.0.20&lt;/a&gt; - This release works all the way up to JDK 20, and also includes a couple of bug fixes!&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_blogs"&gt;Blogs&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/eclipse/eclipse-microservices/how-to-run-artemis-messaging-in-a-bootable-jar/"&gt;How to run Artemis Messaging in a Bootable Jar&lt;/a&gt; - If you’re running a Wildfly application as a bootable jar, you’ll want to take note of this. Running your application as a bootable jar is a great way to get started moving towards containers, or even building a container around the bootable jar.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-newsletter-25/"&gt;Quarkus Newsletter 25&lt;/a&gt; - If you’re looking to catch up with what is happening in Quarkus, look no further. James Cobb gives us a great rundown of things happening in the Quarkus community.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://wildfly-security.github.io/wildfly-elytron/hacktoberfest/"&gt;Hacktoberfest&lt;/a&gt; - There’s still time to get involved in Hacktoberfest! Be sure to check it out and join others in building better Open Source Software.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;p&gt;Thanks everyone for visiting, we hope to see you back next time!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/jason-porter.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Jason Porter&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Jason Porter</dc:creator></entry><entry><title type="html">How to run Artemis Messaging in a Bootable Jar</title><link rel="alternate" href="http://www.mastertheboss.com/eclipse/eclipse-microservices/how-to-run-artemis-messaging-in-a-bootable-jar/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/eclipse/eclipse-microservices/how-to-run-artemis-messaging-in-a-bootable-jar/</id><updated>2022-10-19T16:26:51Z</updated><content type="html">This article is a walk-through the new embedded-activemq layer which allows to run the Artemis MQ Server in a WildFly Bootable Jar. WildFly Bootable Jar allows to run Jakarta EE applications as runnable Jar applications, which include the required subsystems as Galleon layers. You can learn more about WildFly bootable Jar in this article: Turn ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">KOGITO 1.29.0 RELEASED!</title><link rel="alternate" href="https://blog.kie.org/2022/10/kogito-1-29-0-released.html" /><author><name>Ricardo Zanini</name></author><id>https://blog.kie.org/2022/10/kogito-1-29-0-released.html</id><updated>2022-10-19T13:07:47Z</updated><content type="html">We are glad to announce that the Kogito 1.29.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * workflowdata property is no longer needed to start a process (OpenAPI documentation has been properly updated to reflect this) * OpenAPI documentation is properly generated when an application defines several workflows.  * New custom marshaller/unmarshaller policy.  * New kogito-events-core dependency, which contains event related classes that were located in kogito-services and classes located in kogito-events-api which depends on Jackson. As a consequence of this refactor, kogito-events-api no longer depends on Jackson and kogito services dependency no longer contains event related classes.  * CloneHelper class works in native mode.  * Kogito Serverless Workflow Chrome Extension now uses by default the new visualization. BREAKING CHANGES * ProcessDataEvent class constructors have been refactored, keeping only the empty constructor and all arguments constructor. Therefore, any code outside Kogito using the removed constructor will fail to compile. The alternative is to use an empty constructor plus setters or the all arguments constructor, filling the gaps with nulls.  * If a pom.xml file explicitly imports kogito-events-api and kogito services dependencies, it probably needs to be updated to replace them with kogito-events-core.   For more details, head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found. * Kogito images are available on. * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.24.0 artifacts are available at the. A detailed changelog for 1.29.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Ricardo Zanini</dc:creator></entry><entry><title>Open vSwitch: The revalidator process explained</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/10/19/open-vswitch-revalidator-process-explained" /><author><name>Eelco Chaudron</name></author><id>c7147af2-1bce-4304-b987-1015d70a1150</id><updated>2022-10-19T07:00:00Z</updated><published>2022-10-19T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.openvswitch.org"&gt;Open vSwitch&lt;/a&gt; (OVS) is a rich implementation of software-defined networking (SDN) L2 switch. OVS's revalidator process, invoked by Open vSwitch's &lt;a href="http://www.openvswitch.org/support/dist-docs/ovs-vswitchd.8.txt"&gt;ovs-vswitchd&lt;/a&gt; daemon, is responsible for the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Making sure the installed datapath flows match the configured OpenFlow rules&lt;/li&gt; &lt;li&gt;Updating OpenFlow packet and byte statistics&lt;/li&gt; &lt;li&gt;Removing idle datapath flows&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;However, the OVS implementation details are not well documented or understood. The goal of this article is to provide insight into the operation of the revalidator.&lt;/p&gt; &lt;h2&gt;Revalidator threads&lt;/h2&gt; &lt;p&gt;The ovs-vswitchd daemon creates several revalidator threads when the datapath initializes at the creation of the first bridge for the datapath. If you use both the kernel (system) datapath and the userspace (netdev) datapath, two sets of revalidator threads get created.&lt;/p&gt; &lt;p&gt;The number of total threads created depends on the &lt;code&gt;n-revalidator-threads&lt;/code&gt; setting. The documentation states:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-markdown"&gt;other_config : n-revalidator-threads: optional string, containing an integer, at least 1 Attempts to specify the number of threads for software datapaths to use for revalidating flows in the datapath. Some datapaths may choose to ignore this and will set to a sensible option for the datapath type. Typically, there is a direct correlation between the number of revalidator threads, and the number of flows allowed in the datapath. The default is the number of cpu cores divided by four plus one. If n-handler-threads is set, the default changes to the number of cpu cores minus the number of handler threads. This configuration is per datapath. If you have more than one software datapath (e.g. some system bridges and some netdev bridges), then the total number of threads is n-handler-threads times the number of software datapaths.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you would like to change the number of revalidator threads, you can enter commands such as these:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-markdown"&gt;# vswitchd.log | grep n-revalidator 2022-08-30T09:07:09.023Z|00029|ofproto_dpif_upcall|INFO|Overriding n-handler-threads to 17, setting n-revalidator-threads to 5 # pidstat -t -p `pidof ovs-vswitchd` | grep revalidator 05:07:19 AM 976 - 267439 0.00 0.00 0.00 0.00 0.00 12 |__revalidator19 05:07:19 AM 976 - 267440 0.00 0.00 0.00 0.00 0.00 21 |__revalidator20 05:07:19 AM 976 - 267441 0.00 0.00 0.00 0.00 0.00 9 |__revalidator21 05:07:19 AM 976 - 267442 0.00 0.00 0.00 0.00 0.00 0 |__revalidator22 05:07:19 AM 976 - 267443 0.00 0.00 0.00 0.00 0.00 27 |__revalidator23 # ovs-vsctl set Open_vSwitch . other-config:n-revalidator-threads=10 # # cat /var/log/openvswitch/ovs-vswitchd.log | grep n-revalidator 2022-08-30T09:07:09.023Z|00029|ofproto_dpif_upcall|INFO|Overriding n-handler-threads to 17, setting n-revalidator-threads to 5 2022-08-30T09:08:22.216Z|00039|ofproto_dpif_upcall|INFO|Overriding n-handler-threads to 17, setting n-revalidator-threads to 10 # pidstat -t -p `pidof ovs-vswitchd` | grep revalidator 05:08:29 AM 976 - 267543 0.00 0.00 0.00 0.00 0.00 27 |__revalidator41 05:08:29 AM 976 - 267544 0.00 0.00 0.00 0.00 0.00 7 |__revalidator42 05:08:29 AM 976 - 267545 0.00 0.00 0.00 0.00 0.00 10 |__revalidator43 05:08:29 AM 976 - 267546 0.00 0.00 0.00 0.00 0.00 14 |__revalidator44 05:08:29 AM 976 - 267547 0.00 0.00 0.00 0.00 0.00 26 |__revalidator45 05:08:29 AM 976 - 267548 0.00 0.00 0.00 0.00 0.00 11 |__revalidator46 05:08:29 AM 976 - 267549 0.00 0.00 0.00 0.00 0.00 21 |__revalidator47 05:08:29 AM 976 - 267550 0.00 0.00 0.00 0.00 0.00 24 |__revalidator48 05:08:29 AM 976 - 267551 0.00 0.00 0.00 0.00 0.00 12 |__revalidator49 05:08:29 AM 976 - 267552 0.00 0.00 0.00 0.00 0.00 9 |__revalidator50 &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;How the revalidator threads operate&lt;/h2&gt; &lt;p&gt;What do the revalidator threads do? The following comment in Open vSwitch's &lt;code&gt;ofproto-dpif-upcall.c&lt;/code&gt; file provides an initial answer:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt; /* A thread that processes datapath flows, updates OpenFlow statistics, and * updates or removes them if necessary. * * Revalidator threads operate in two phases: "dump" and "sweep". In between * each phase, all revalidators sync up so that all revalidator threads are * either in one phase or the other, but not a combination. * * During the dump phase, revalidators fetch flows from the datapath and * attribute the statistics to OpenFlow rules. Each datapath flow has a * corresponding ukey which caches the most recently seen statistics. If * a flow needs to be deleted (for example, because it is unused over a * period of time), revalidator threads may delete the flow during the * dump phase. The datapath is not guaranteed to reliably dump all flows * from the datapath, and there is no mapping between datapath flows to * revalidators, so a particular flow may be handled by zero or more * revalidators during a single dump phase. To avoid duplicate attribution * of statistics, ukeys are never deleted during this phase. * * During the sweep phase, each revalidator takes ownership of a different * slice of umaps and sweeps through all ukeys in those umaps to figure out * whether they need to be deleted. During this phase, revalidators may * fetch individual flows which were not dumped during the dump phase to * validate them and attribute statistics. */&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This all happens in the revalidator threads created by &lt;code&gt;udpif_start_threads()&lt;/code&gt;. The entry point for all these threads is &lt;code&gt;udpif_revalidator()&lt;/code&gt;. I will describe this later in the &lt;a href="#triggering_revalidation"&gt;"Triggering revalidation"&lt;/a&gt; section.&lt;/p&gt; &lt;p&gt;The leader, the first revalidator thread, decides when to run a new dump and sweep. So how does it determine when to run a new dump and sweep? The following criteria determine the answer:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The minimum time between two runs is 5ms.&lt;/li&gt; &lt;li&gt;The delay time is the lesser value of the &lt;code&gt;ofproto_max_idle&lt;/code&gt; and &lt;code&gt;ofproto_max_revalidator&lt;/code&gt; variables.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;ofproto_max_idle&lt;/code&gt; value is fixed and, by default, is 10,000ms when not configured. To configure it, set the &lt;code&gt;max-idle&lt;/code&gt; variable in the &lt;code&gt;Open_vSwitch&lt;/code&gt; database. For example: &lt;pre&gt; &lt;code class="language-bash"&gt;$ ovs-vsctl set Open_vSwitch . other_config:max-idle=20000&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;The &lt;code&gt;ofproto_max_revalidator&lt;/code&gt; value is fixed and, by default, is 500ms when not configured. To configure it, set the &lt;code&gt;max-revalidator&lt;/code&gt; variable in the &lt;code&gt;Open_vSwitch&lt;/code&gt; database. For example: &lt;pre&gt; &lt;code class="language-bash"&gt;$ ovs-vsctl set Open_vSwitch . other_config:max-revalidator=1000&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The documentation describes these two parameters as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-markdown"&gt;other_config : max-idle: optional string, containing an integer, at least 500 The maximum time (in ms) that idle flows will remain cached in the datapath. Internally OVS will check the validity and activ‐ ity for datapath flows regularly and may expire flows quicker than this number, based on real time network conditions. Tweak‐ ing this value is discouraged unless you know exactly what you're doing. The default is 10000.&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-markdown"&gt;other_config : max-revalidator: optional string, containing an integer, at least 100 The maximum time (in ms) that revalidator threads will wait be‐ fore executing flow revalidation. Note that this is maximum al‐ lowed value. Actual timeout used by OVS is minimum of max-idle and max-revalidator values. Tweaking this value is discouraged unless you know exactly what you're doing. The default is 500.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If the timer has expired or the thread is awakened earlier by a &lt;code&gt;udpif-&gt;reval_seq&lt;/code&gt; change, the dump phase starts, as explained in the previous documentation. The &lt;code&gt;udpif-&gt;reval_seq&lt;/code&gt; change happens through a call to &lt;code&gt;udpif_revalidate()&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The leader thread has the following tasks. All other threads will remain idle during these calculations:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Determine the &lt;code&gt;flow_limit&lt;/code&gt;. This value is dynamically adjusted based on the configured &lt;code&gt;other_config:flow-limit&lt;/code&gt; value (default 200,000) and the time the most recent revalidator round took. The initial setting is the lesser value of the configured &lt;code&gt;other_config:flow-limit&lt;/code&gt; and 10,000. Here are the formulas for resetting &lt;code&gt;flow_limit&lt;/code&gt;: &lt;pre&gt; &lt;code class="language-python"&gt; if duration &gt; 2000 ms: flow_limit /= (duration in seconds) elif duration &gt; 1300 ms: flow_limit *= 0.75 elif duration &lt; 1000 ms AND flow_limit &lt; n_flows__last_run * 1000 / duration: flow_limit += 1000 flow_limit = MIN(other_config:flow-limit, MAX(flow_limit, 1000))&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Get the current number of flows in the datapath and set &lt;code&gt;n_flows&lt;/code&gt; to that value.&lt;/li&gt; &lt;li&gt;Set the &lt;code&gt;max_n_flows&lt;/code&gt; to the maximum ever seen: That is, set &lt;code&gt;max_n_flows&lt;/code&gt; to &lt;code&gt;n_flows&lt;/code&gt; if &lt;code&gt;n_flows&lt;/code&gt; is greater.&lt;/li&gt; &lt;li&gt;Reset &lt;code&gt;avg_n_flows&lt;/code&gt; to reflect the number of flows at the current run: In other words, set it to (&lt;code&gt;avg_n_flows&lt;/code&gt; + &lt;code&gt;n_flows&lt;/code&gt;) / 2.&lt;/li&gt; &lt;li&gt;Create a flow dump. The dump is in terse mode if the datapath enables and supports unique flow identifiers &lt;em&gt;(&lt;/em&gt;UFIDs). Otherwise, a full dump happens. A dump in terse mode returns only the UFID and statistics. The following comment from &lt;code&gt;dpif.h&lt;/code&gt; explains the sequence of events and functions called during a flow dump:&lt;/li&gt; &lt;/ol&gt;&lt;pre&gt; &lt;code class="language-cpp"&gt; /* Flow dumping interface * ====================== * * This interface allows iteration through all of the flows currently installed * in a datapath. It is somewhat complicated by two requirements: * * - Efficient support for dumping flows in parallel from multiple threads. * * - Allow callers to avoid making unnecessary copies of data returned by * the interface across several flows in cases where the dpif * implementation has to maintain a copy of that information anyhow. * (That is, allow the client visibility into any underlying batching as * part of its own batching.) * * * Usage * ----- * * 1. Call dpif_flow_dump_create(). * 2. In each thread that participates in the dump (which may be just a single * thread if parallelism isn't important): * (a) Call dpif_flow_dump_thread_create(). * (b) Call dpif_flow_dump_next() repeatedly until it returns 0. * (c) Call dpif_flow_dump_thread_destroy(). * 3. Call dpif_flow_dump_destroy(). * * All error reporting is deferred to the call to dpif_flow_dump_destroy(). */&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: The upcoming &lt;a href="#using_upcall_show"&gt;"Using upcall/show to see internal variables"&lt;/a&gt; section explains how to view the values of these dynamic variables.&lt;/p&gt; &lt;p&gt;After the tasks are complete, the leader thread is the last thread to hit &lt;code&gt;ovs_barrier_block()&lt;/code&gt;, following which all revalidator threads continue by calling &lt;code&gt;revalidate()&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;code&gt;revalidate()&lt;/code&gt; does the following:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Call &lt;code&gt;dpif_flow_dump_thread_create&lt;/code&gt;().&lt;/li&gt; &lt;li&gt;Call &lt;code&gt;dpif_flow_dump_next&lt;/code&gt;() to get a maximum of &lt;code&gt;REVALIDATE_MAX_BATCH&lt;/code&gt; (50) flows, and return if zero flows are received.&lt;/li&gt; &lt;li&gt;Determine the &lt;code&gt;max_idle&lt;/code&gt; timeout.&lt;/li&gt; &lt;li&gt;For each batch where &lt;code&gt;n_dp_flows&lt;/code&gt; &gt;= &lt;code&gt;flow_limit&lt;/code&gt;, increment the &lt;code&gt;upcall_flow_limit_hit&lt;/code&gt; coverage counter.&lt;/li&gt; &lt;li&gt;For each flow in the batch, follow these five steps: &lt;ol&gt;&lt;li&gt; &lt;p&gt;Call &lt;code&gt;ukey_acquire()&lt;/code&gt;, which tries to find the &lt;code&gt;(struct udpif_key *) ukey&lt;/code&gt; corresponding to the datapath flow.&lt;/p&gt; &lt;p&gt;If ukey exists but not locked, then some other thread is using it, and the revalidation for this round is skipped. If the error is &lt;code&gt;EBUSY&lt;/code&gt; (can't take the lock), the function increases the &lt;code&gt;upcall_ukey_contention&lt;/code&gt; coverage counter. All other acquisition errors call &lt;code&gt;log_unexpected_flow()&lt;/code&gt; and add a delete operation for this flow, named &lt;code&gt;delete_op_init__()&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;If the ukey is not found at all, an entry is created so that the ukey will be re-evaluated. This algorithm handles the case where OVS restarts, and we are not blindly deleting all the datapath flows.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Based on the &lt;code&gt;ukey-&gt;dump_seq&lt;/code&gt; value, determine whether we have already looked at this entry. If so, increment one of two overage counters (&lt;code&gt;dumped_duplicate_flow&lt;/code&gt; or &lt;code&gt;dumped_new_flow&lt;/code&gt;) and continue to the next datapath flow.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;If the &lt;code&gt;ukey state &lt;= UKEY_OPERATIONAL&lt;/code&gt;, change the value to &lt;code&gt;UKEY_OPERATIONAL&lt;/code&gt;. If the value is larger than &lt;code&gt;UKEY_OPERATIONAL&lt;/code&gt;, the entry is in some delete state. Log the information and continue with the next entry.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;If the flow has been idle for &lt;code&gt;max_idle&lt;/code&gt; milliseconds or we are in recovery mode, mark the ukey state as &lt;code&gt;UKEY_DELETE&lt;/code&gt;. Otherwise, call &lt;code&gt;revalidate_ukey()&lt;/code&gt;, which marks the ukey state as &lt;code&gt;UKEY_KEEP&lt;/code&gt;, &lt;code&gt;UKEY_DELETE&lt;/code&gt;, or &lt;code&gt;UKEY_MODIFY&lt;/code&gt;. See the  &lt;a href="#revalidate_ukey"&gt;"The revalidate_ukey function"&lt;/a&gt; section for more details.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;If the ukey state is not marked as &lt;code&gt;UKEY_KEEP—&lt;/code&gt;that is, the state is &lt;code&gt;UKEY_DELETE&lt;/code&gt; or &lt;code&gt;UKEY_MODIFY—&lt;/code&gt;add the desired datapath operation to the operation queue. For &lt;code&gt;UKEY_DELETE&lt;/code&gt;, also set the ukey state to &lt;code&gt;UKEY_EVICTING&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Now that the batch is completed, push all the operations to the datapath by calling &lt;code&gt;push_dp_ops()&lt;/code&gt;. For &lt;code&gt;UKEY_DELETE&lt;/code&gt;, this step also updates the statistics and sets the ukey state to &lt;code&gt;UKEY_EVICTED&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Try to receive another batch of flows and repeat this sequence on them. If no more flows are retrieved, continue:&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The following comment from &lt;code&gt;ofproto-dpif-upcall.c&lt;/code&gt; explains the duration of the flows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt; /* In normal operation we want to keep flows around until they have * been idle for 'ofproto_max_idle' milliseconds. However: * * - If the number of datapath flows climbs above 'flow_limit', * drop that down to 100 ms to try to bring the flows down to * the limit. * * - If the number of datapath flows climbs above twice * 'flow_limit', delete all the datapath flows as an emergency * measure. (We reassess this condition for the next batch of * datapath flows, so we will recover before all the flows are * gone.) */&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: The number of datapath flows compared for each batch, but the number is read from the datapath only every 100ms. So these actions might not be taken on all the flows.&lt;/p&gt; &lt;p&gt;At this point, the thread has exited from &lt;code&gt;revalidate()&lt;/code&gt;. It waits until all threads finish &lt;code&gt;revalidate()&lt;/code&gt;. Once this happens, the sweep phase starts with a call to &lt;code&gt;revalidator_sweep()&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;revalidator_sweep()&lt;/code&gt;function carries out the following tasks:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Loop over all ukey entries in the threads' responsible umaps. For each entry, follow these three steps: &lt;ol&gt;&lt;li&gt;Try to lock the ukey's mutex. If the lock cannot be taken, skip the entry in this round.&lt;/li&gt; &lt;li&gt;If the ukey state is &lt;code&gt;UKEY_OPERATIONAL&lt;/code&gt; and the entry missed the &lt;code&gt;revalidate_ukey()&lt;/code&gt; call or dump phase, call &lt;code&gt;revalidate_ukey()&lt;/code&gt; and &lt;code&gt;push_dp_ops()&lt;/code&gt;, just as in the dump phase.&lt;/li&gt; &lt;li&gt;If the new ukey state is &lt;code&gt;UKEY_EVICTED&lt;/code&gt;, call &lt;code&gt;ukey_delete()&lt;/code&gt;, which removes the ukey from the umap, sets the ukey state to &lt;code&gt;UKEY_DELETED&lt;/code&gt;, and frees the data structure using Linux's read, copy, update (RCU) system.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Now we are done until the next revalidate timeout or &lt;code&gt;udpif-&gt;reval_seq&lt;/code&gt; change, which starts another dump and sweep phase.&lt;/p&gt; &lt;h2&gt;The revalidate_ukey function&lt;/h2&gt; &lt;p&gt;If needed, this function revalidates the datapath flow and updates the OpenFlow statistics.&lt;/p&gt; &lt;p&gt;The first thing this function does is make sure we need to revalidate flows. OpenFlow revalidation is triggered through a call to &lt;code&gt;udpif_revalidate()&lt;/code&gt;, as described in the upcoming section, &lt;a href="#triggering_revalidation"&gt;"Triggering revalidation."&lt;/a&gt; The decision is made by comparing &lt;code&gt;udpif-&gt;reval_seq&lt;/code&gt; with u&lt;code&gt;key-&gt;reval_seq&lt;/code&gt;. If they are different, the system needs a revalidation.&lt;/p&gt; &lt;p&gt;Let's explain this function with this pseudocode:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-markdown"&gt;def revalidate_ukey(udpif, ukey): result = UKEY_DELETE IF udpif-&gt;reval_seq != ukey-&gt;reval_seq: IF !used #[1] OR duration &lt; (other_config:max-revalidator / 2) #[2] OR flow_pps &gt; other_config:min-revalidate-pps: #[3] IF xlate_ukey(): IF ukey-&gt;actions == xlate_actions: result = UKEY_KEEP ELSE: result = UKEY_UPDATE ELSE: # Delete ukey as it can not be translated ELSE: # Delete ukey, to expensive to revalidate ELIF !n_packets OR ukey-&gt;xcache OR !populate_xcache(): result = UKEY_KEEP IF result != UKEY_DELETE: update_statistics #[4] ukey-&gt;reval_seq = udpif-&gt;reval_seq RETURN result&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following list explains the lines marked by numbers on the right side of the code:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;When &lt;code&gt;used&lt;/code&gt; is set, the datapath flow was programmed, but no data has passed through it.&lt;/li&gt; &lt;li&gt;This calculation determines whether it's likely that the function can do the whole revalidation in &lt;code&gt;max-revalidator&lt;/code&gt; time.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;other_config:min-revalidate-pps&lt;/code&gt; configuration option is documented as follows: &lt;pre&gt; &lt;code class="language-markdown"&gt;other_config : min-revalidate-pps: optional string, containing an integer, at least 1 Set minimum pps that flow must have in order to be revalidated when revalidation duration exceeds half of max-revalidator con‐ fig variable. The default is 5.&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Another part of the code updates the statistics for the &lt;code&gt;UKEY_DELETE&lt;/code&gt; state.&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;/ul&gt;&lt;h2&gt;Triggering revalidation&lt;/h2&gt; &lt;p&gt;As seen before, calling &lt;code&gt;udpif_revalidate()&lt;/code&gt; can trigger a revalidation, which just issues the call &lt;code&gt;seq_change(udpif-&gt;reval_seq)&lt;/code&gt;. &lt;code&gt;udpif_revalidate()&lt;/code&gt; is called only from the &lt;code&gt;type_run()&lt;/code&gt; function in &lt;code&gt;ofproto-dpif.c&lt;/code&gt;, and is called only if &lt;code&gt;backer-&gt;need_revalidate&lt;/code&gt; is non-zero. This value also indicates the necessity of revalidation. However, the value might not be 100% accurate because multiple revalidates could be triggered before &lt;code&gt;type_run()&lt;/code&gt; is called.&lt;/p&gt; &lt;p&gt;&lt;code&gt;ofproto-dpif.h&lt;/code&gt; defines the following revalidation reasons:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;enum revalidate_reason { REV_RECONFIGURE = 1, /* Switch configuration changed. */ REV_STP, /* Spanning tree protocol port status change. */ REV_RSTP, /* RSTP port status change. */ REV_BOND, /* Bonding changed. */ REV_PORT_TOGGLED, /* Port enabled or disabled by CFM, LACP, ...*/ REV_FLOW_TABLE, /* Flow table changed. */ REV_MAC_LEARNING, /* Mac learning changed. */ REV_MCAST_SNOOPING, /* Multicast snooping changed. */ };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For each type, a corresponding coverage counter might help administrators debug why revalidation happens. Note that only non-zero coverage counters are shown in the following example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-markdown"&gt; # ovs-appctl coverage/show | grep -E "rev_|Event" Event coverage, avg rate over last: 5 seconds, last minute, last hour, hash=d9180580: rev_port_toggled 0.0/sec 0.000/sec 0.0000/sec total: 1 rev_flow_table 0.0/sec 0.000/sec 2.7761/sec total: 9995&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Using upcall/show to see internal variables&lt;/h2&gt; &lt;p&gt;The variables we discussed in the previous sections can be read using the &lt;code&gt;ovs-appctl&lt;/code&gt; command. Here is a sample output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ovs-appctl upcall/show system@ovs-system: flows : (current 10000) (avg 9999) (max 10000) (limit 172000) dump duration : 9ms ufid enabled : true 19: (keys 2001) 20: (keys 1956) 21: (keys 1980) 22: (keys 1982) 23: (keys 2081)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These values are related to the current run, or the last run if the system is waiting for the next run to start. The displayed names match up with internal variables as follows:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;current&lt;/code&gt;: &lt;code&gt;n_flows&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;avg&lt;/code&gt;: &lt;code&gt;avg_n_flows&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;max&lt;/code&gt;: &lt;code&gt;max_n_flows&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;limit&lt;/code&gt;: Dynamically calculated &lt;code&gt;flow_limit&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dump duration&lt;/code&gt;: The total time of the most recent dump and sweep phase&lt;/li&gt; &lt;li&gt;&lt;code&gt;&lt;em&gt;xx&lt;/em&gt;: (keys &lt;nm&gt;)&lt;/code&gt;: &lt;em&gt;xx&lt;/em&gt; is the OVS thread ID of the revalidator thread, which you can see with one of the following commands: &lt;pre&gt; &lt;code class="language-markdown"&gt;# pidstat -t -p `pidof ovs-vswitchd` # top -H -o +%CPU -p $(pidof ovs-vswitchd) -b -n 1&lt;/code&gt;&lt;/pre&gt; The &lt;code&gt;keys&lt;/code&gt; value represents the number of ukeys mapped to this thread's umaps.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Getting insights over time&lt;/h2&gt; &lt;p&gt;Now that we know all of the internals, it would be nice to visualize some of them. Recently, a &lt;a href="https://patchwork.ozlabs.org/project/openvswitch/list/?series=319156"&gt;patch&lt;/a&gt; introduced some user statically-defined tracing (USDT) probes and a script to enable this visualization.&lt;/p&gt; &lt;p&gt;The enabled script shows some real-time plots of the various variables. Figure 1 shows an example. The dot on the &lt;code&gt;n_flow&lt;/code&gt; graph indicates a triggered run.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/plot1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/plot1.png?itok=LuPlJj2p" width="1440" height="758" alt="A plot of actual_time shows spikes and sudden plunges." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A plot of actual_time shows spikes and sudden plunges. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;p&gt;Here is an explanation of some of the events shown in the graph:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The first drop in &lt;code&gt;actual_time&lt;/code&gt; took place because we added an OpenFlow rule to the existing 1,000. Immediately after that drop, all existing 1,000 flows were invalidated, taking about 9ms, as shown in the line for &lt;code&gt;n_reval_flows&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;The second drop of &lt;code&gt;actual_time&lt;/code&gt; came because I added 1,000 flows with individual &lt;code&gt;ovs-vsctl&lt;/code&gt; commands, causing a lot of &lt;code&gt;udpif_revalidate()&lt;/code&gt; invocations (i.e., short instances of &lt;code&gt;actual_wait&lt;/code&gt;). In addition, this change caused the &lt;code&gt;flow_limit&lt;/code&gt; to grow further.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;When you exit the script, it shows a similar graph reflecting data from the full run. Figure 2 shows an example.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/plot2.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/plot2.png?itok=epWXgx1b" width="1440" height="758" alt="The final plot displays data from the full run." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The final plot displays data from the full run. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;p&gt;There is also an option to save all the events and run the script offline to generate the graphs. In addition, the script always dumps the statistics to the console in CSV format:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./reval_monitor.py -r captured_reval.events - Reading events from "captured_reval.events"... ts_start, ts_complete, n_flows, n_reval_flows, avg_n_flows, max_n_flows, flow_limit, dump_duration, poll_wait, actual_wait 344704473543451, 344704473963582, 1000, 0, 999, 1000, 67000, 1, 500, 500.62 344704974588221, 344704975002936, 1000, 0, 999, 1000, 68000, 1, 500, 499.62 344705474624368, 344705475047878, 1000, 0, 999, 1000, 69000, 1, 500, 499.62 344705974669681, 344705981642974, 1000, 0, 999, 1000, 70000, 7, 500, 493.57 344706475208230, 344706475330465, 0, 0, 499, 1000, 70000, 1, 500, 500.52 344706975854978, 344706975929932, 0, 0, 249, 1000, 70000, 1, 500, 500.45 344707476384490, 344707476431131, 0, 0, 124, 1000, 70000, 1, 500, 500.53 344707976957291, 344707977003421, 0, 0, 62, 1000, 70000, 1, 500, 499.45 344708476457462, 344708476503563, 0, 0, 31, 1000, 70000, 1, 500, 500.53 344708977030540, 344708977078802, 0, 0, 15, 1000, 70000, 1, 500, 500.52 344709477603353, 344709477650819, 0, 0, 7, 1000, 70000, 1, 500, 107.16 344709584813142, 344709584856818, 0, 0, 3, 1000, 70000, 1, 500, 5.19 344709590051075, 344709590093409, 0, 0, 1, 1000, 70000, 1, 500, 5.18 344709595270855, 344709595313071, 0, 0, 0, 1000, 70000, 1, 500, 5.18 344709600488455, 344709600530112, 0, 0, 0, 1000, 70000, 1, 500, 5.18 344709605705511, 344709605748460, 0, 0, 0, 1000, 70000, 1, 500, 5.15&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; &lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;You should now be able to analyze the datapath flow behavior of &lt;code&gt;ovs-vswitchd&lt;/code&gt; on any running system. This article was inspired because the OVS implementation details are not well documented or understood. We hope we have provided clarity and insight into the operation of the revalidator. Please feel free to comment below. We always welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/10/19/open-vswitch-revalidator-process-explained" title="Open vSwitch: The revalidator process explained"&gt;Open vSwitch: The revalidator process explained&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Eelco Chaudron</dc:creator><dc:date>2022-10-19T07:00:00Z</dc:date></entry><entry><title>Quarkus 2.13.3.Final released - Maintenance release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-13-3-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-13-3-final-released/</id><updated>2022-10-19T00:00:00Z</updated><published>2022-10-19T00:00:00Z</published><summary type="html">Today, we released Quarkus 2.13.3.Final with a new round of bugfixes and documentation improvements. It is a recommended and safe upgrade for anyone already using 2.13. Especially since it fixes a Jackson CVE (even if you probably are not affected by the CVE, better be safe). If you are not...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2022-10-19T00:00:00Z</dc:date></entry><entry><title>An API-first approach to building Node.js applications</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/10/18/api-first-approach-building-nodejs-applications" /><author><name>Wojciech Trocki</name></author><id>d0c6ef97-2223-46db-aef9-292c01f8dc7b</id><updated>2022-10-18T07:00:00Z</updated><published>2022-10-18T07:00:00Z</published><summary type="html">&lt;p&gt;API services are a common component of &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; applications. This installment of the ongoing &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js reference architecture&lt;/a&gt; series focuses on the development of REST &lt;a href="https://developers.redhat.com/topics/api-management/"&gt;APIs&lt;/a&gt; using an API-first approach.&lt;/p&gt; &lt;p&gt;Read the series so far:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Part 1: &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview"&gt;Overview of the Node.js reference architecture&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 2: &lt;a href="https://developer.ibm.com/blogs/nodejs-reference-architectire-pino-for-logging/"&gt;Logging in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2021/05/17/introduction-nodejs-reference-architecture-part-3-code-consistency"&gt;Code consistency in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 4: &lt;a href="https://developers.redhat.com/articles/2021/06/22/introduction-nodejs-reference-architecture-part-4-graphql-nodejs"&gt;GraphQL in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 5: &lt;a href="https://developers.redhat.com/articles/2021/08/26/introduction-nodejs-reference-architecture-part-5-building-good-containers"&gt;Building good containers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 6: &lt;a href="https://developers.redhat.com/articles/2021/12/03/introduction-nodejs-reference-architecture-part-6-choosing-web-frameworks"&gt;Choosing web frameworks&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 7: &lt;a href="https://developers.redhat.com/articles/2022/03/02/introduction-nodejs-reference-architecture-part-7-code-coverage"&gt;Code coverage&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 8: &lt;a href="https://developers.redhat.com/articles/2022/04/11/introduction-nodejs-reference-architecture-part-8-typescript"&gt;Typescript&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 9: &lt;a href="https://developers.redhat.com/articles/2022/08/09/8-elements-securing-nodejs-applications"&gt;Securing Node.js applications&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 10: &lt;strong&gt;API-first approach to building Node.js applications&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The IBM and Red Hat team has provided a &lt;a href="https://nodeshift.dev/nodejs-reference-architecture/functional-components/apifirst"&gt;comprehensive reference for developers looking to build Node.js applications&lt;/a&gt;. The Markdown source file of the reference specification is also &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/functional-components/rest-api-development.md"&gt;available on GitHub&lt;/a&gt; under the Apache 2.0 open source license.&lt;/p&gt; &lt;p&gt;In the typical process of building API services, developers start by implementing their endpoints and exposing them using a REST API. After the API is deployed, the team often faces challenges documenting and sharing information about that API. This article lays out the most common approaches to designing and implementing REST APIs, culminating in what we call the API-first approach.&lt;/p&gt; &lt;h2&gt;Methods for building a REST API&lt;/h2&gt; &lt;p&gt;Teams building and documenting a REST API usually employ one of the following methods:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Create the API implementation first, then manually add documentation as a document, web page, or OpenAPI spec. Changes and updates call for special attention to keep endpoints and documentation in sync, because documentation cannot be automatically validated. The quality of the documentation depends on the attention to detail given by the developers who maintain it.&lt;/li&gt; &lt;li&gt;Annotate code in order to generate an OpenAPI spec for the consumers served by the REST endpoints. This approach is often called "code-first" because the OpenAPI file is created by processing information in the code.&lt;/li&gt; &lt;li&gt;Generate an OpenAPI file during development and publish it to the source repository. The specification file is accessible without running the service. However, clients and integrators cannot modify the specification in order to propose new changes.&lt;/li&gt; &lt;li&gt;Create an OpenAPI file as input to the API design process. The file can be used as a scaffold for different clients, SDKs, command-line interfaces (CLIs), and even backends. This approach is called "API-first" or "schema-first" and is being widely adopted by teams building large sets of APIs for a wide range of clients.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Each of these methods represents a different maturity level in regard to maintenance, discoverability, and automation. Table 1 lists the key traits of each method.&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="500"&gt;&lt;caption&gt;Table 1: There are four levels in the API maturity model.&lt;/caption&gt; &lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;1. Manual documentation&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;2. OpenAPI from code&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;3. OpenAPI in code&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;4. OpenAPI first&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;ul&gt;&lt;li&gt;Prone to errors, missing arguments etc.&lt;/li&gt; &lt;li&gt;Hard to update and maintain&lt;/li&gt; &lt;li&gt;Weak API contract due to manual API design&lt;/li&gt; &lt;li&gt;Limited paths to automation&lt;/li&gt; &lt;li&gt;Limited ways to report issues, propose new changes, or improve API&lt;/li&gt; &lt;li&gt;Clients need to be written by hand&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;OpenAPI spec is as good as code annotations provided by developers&lt;/li&gt; &lt;li&gt;Generated OpenAPI file missing details and it is often suboptimal for client generation&lt;/li&gt; &lt;li&gt;Lack of visibility for various stakeholders before API is implemented and deployed&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;Simple automation of API client generation&lt;/li&gt; &lt;li&gt;Simple implementation of API validation using tools like Spectral&lt;/li&gt; &lt;li&gt;No ability to propose changes and edit generated file&lt;/li&gt; &lt;li&gt;Requires CI/CD workflow to keep file up to date&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;Ability to negotiate API at the design level&lt;/li&gt; &lt;li&gt;Possibility to use API design tools like Apicurio Studio, Swagger Editor, etc.&lt;/li&gt; &lt;li&gt;Workflow focused on code generation and scaffolding, minimizing errors and ensuring a strong API&lt;/li&gt; &lt;li&gt;Editable OpenAPI file in the repository&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;What is an API contract and why is it important?&lt;/h2&gt; &lt;p&gt;An API contract is a qualitative measure of how well an API's documentation reflects the real state of the API. The contract is strong if the documentation is up-to-date and accurate, and weak when the documentation diverges significantly from the real service.&lt;/p&gt; &lt;p&gt;A strong API contract meets the following criteria:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Adopters have a full list of operations, inputs, and outputs documented by an OpenAPI file or other standards.&lt;/li&gt; &lt;li&gt;Developers can track and detect breaking changes.&lt;/li&gt; &lt;li&gt;The API is properly versioned using semantic versioning, such as X.Y.Z to represent major version (X), minor version (Y), and release (Z).&lt;/li&gt; &lt;li&gt;The API describes and defines all error states. An error state is an inconsistent or invalid situation, and should not be confused with an HTTP status code.&lt;/li&gt; &lt;li&gt;The API contains examples that are up to date and reflect all possible states of the API.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Each level of the API maturity model in Table 1 provides a stronger API contract than the previous ones.&lt;/p&gt; &lt;h2&gt;API-first approach&lt;/h2&gt; &lt;p&gt;In the API-first approach, the highest level of the API maturity model, designing the API is the first priority and is done before writing any code. The approach involves thorough thinking and planning through collaboration with different teams on both the client and backend sides.&lt;/p&gt; &lt;p&gt;A good design process results in high-level documentation describing the intent of the API. One important impact is that API consumers can build clients without waiting for the server to be finished.&lt;/p&gt; &lt;p&gt;This API contract acts as a central draft keeping all your team members aligned on your API's objectives and on how to expose your API's resources. The finalization of the contract allows the team to build the interface of the application.&lt;/p&gt; &lt;p&gt;After the interface is published, cross-functional teams rely on it to build the rest of the application, independent of each other. In practice, teams can generate backend stubs and fully functional client libraries to avoid deviating from the specification in the OpenAPI file.&lt;/p&gt; &lt;h2&gt;Code-first versus API-first&lt;/h2&gt; &lt;p&gt;The code-first approach provides libraries that understand the server-side backend structure and generates respective OpenAPI files. Full control over API lies within the server-side team. Therefore, the generated OpenAPI file is read-only and cannot be effectively used by client developers to negotiate API features with the server-side team.&lt;/p&gt; &lt;p&gt;Although the code-first approach is often seen as good practice and is adopted by many teams, it doesn't provide a very strong API contract. Code doesn't contain enough metadata and information to generate compatible documentation. Concepts in the code, such as inheritance, are often not well reflected in OpenAPI.&lt;/p&gt; &lt;p&gt;The API-first approach uses OpenAPI file as the source of truth. Both client-side and server-side developers write code based on the OpenAPI file. API-first gives the unique ability for clients and backends to start development at the same time at the cost of initial API design and validation phases.&lt;/p&gt; &lt;p&gt;After many experiments and implementations using both methods, the Node.js reference architecture team recommends the API-first approach. The &lt;a href="https://nodeshift.dev/nodejs-reference-architecture/functional-components/apifirst"&gt;reference document&lt;/a&gt; mentioned at the beginning of this article includes a recommendation for following an API-first approach when building APIs.&lt;/p&gt; &lt;h2&gt;The API-first development process&lt;/h2&gt; &lt;p&gt;The API-first approach might lead to modifications in your team's development workflow. Figure 1 shows a typical workflow for developing a REST API.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1_18.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig1_18.png?itok=O8q2eOJr" width="600" height="307" alt="Diagram illustrating that a development workflow covers design, validate, generate, implement, and release phases." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A development workflow covers design, validate, generate, implement, and release phases. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Developers can design and propose new versions of the API as GitHub pull requests (PRs) or GitLab merge requests, which can be peer-reviewed. Proposed changes can also be validated by PR builds, GitHub actions, etc. Some teams use external systems such as the &lt;a href="https://www.apicur.io/registry/"&gt;Apicurio Registry&lt;/a&gt; to hold proposed versions of the schemas and to facilitate early adoption across teams.&lt;/p&gt; &lt;p&gt;Once an API is approved, it can be used to generate early artifacts that enable both client and backend developers to start implementing features backed by the new API. Every correction of the API specification restarts the process and automatically notifies every developer about the API changes. Once all development is finished, code changes along with the API definitions are merged to the main branch in order to be released to the API consumers.&lt;/p&gt; &lt;h2&gt;Challenges with the API-first approach&lt;/h2&gt; &lt;p&gt;Adhering to an API-first workflow requires a certain discipline from teams, calling on them to think about API consumers and business requirements when building the API specification. This high-level thinking helps keep the API free from implementation details, but introduces additional overhead on the implementation of the backend. An API-first approach also requires a wider group of API stakeholders to understand OpenAPI specifications.&lt;/p&gt; &lt;p&gt;Writing OpenAPI using JSON and YAML is often error-prone. Teams can avoid errors by using web editors or VScode plugins to help designers validate API during the design process.&lt;/p&gt; &lt;p&gt;Introducing API-first into an organization often requires changes to its development model. API-first delays feature development until the design is finished and the API is reviewed and validated.&lt;/p&gt; &lt;p&gt;The API-first process requires engagement from different teams and tools. The method is being adopted by teams who have a wide range of experience with building &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt; workflows that can automate API validation. API-first works well for teams that use code-sharing repositories such as GitHub or GitLab for the code and OpenAPI review process.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;We hope we've gotten you excited about using an API-first approach on your next Node.js project and given you the tools to get started. For specific recommendations, check out the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/functional-components/rest-api-development.md"&gt;REST API Development section&lt;/a&gt; in the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/"&gt;Node.js reference architecture&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We plan to cover new topics regularly as part of the &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/"&gt;Node.js reference architecture series&lt;/a&gt;. Until the next installment, we invite you to visit the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js reference architecture repository&lt;/a&gt; on GitHub, where you will see the work we have done and look forward to future topics. To learn more about what Red Hat is up to on the Node.js front, check out our &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/10/18/api-first-approach-building-nodejs-applications" title="An API-first approach to building Node.js applications"&gt;An API-first approach to building Node.js applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Wojciech Trocki</dc:creator><dc:date>2022-10-18T07:00:00Z</dc:date></entry><entry><title type="html">Drools 8 Final &amp;#8211; toward a modular and cloud native rule engine</title><link rel="alternate" href="https://blog.kie.org/2022/10/drools-8-final-toward-a-modular-and-cloud-native-rule-engine.html" /><author><name>Mario Fusco</name></author><id>https://blog.kie.org/2022/10/drools-8-final-toward-a-modular-and-cloud-native-rule-engine.html</id><updated>2022-10-17T12:30:20Z</updated><content type="html">The Drools team is happy to announce that our rule engine reached the 8 Final release. Drools 8-series is a natural evolution of the 7.x-stream, incorporating many features and lessons learned integrating with Kogito and many cloud-native use cases. The purpose of this article is to give a high level overview of the features introduced with this new major release and give some insights on the next developments on which we are working. Let us know your feedback while using these new Drools 8 capabilities, as we are iterating our future plans on them! DEVELOPER EXPERIENCE WRITE RULES IN PURE JAVA (OR BRING-YOUR-OWN-LANGUAGE)! While you know you have learned to love our very own Drools Rule Language (DRL), we also understand how hard can it be devoting time to learn a new programming language, as close to Java it may be! We hear you! This is why, with Drools 8, we are introducing a to write rules without leaving the comfort of your traditional IDE. In fact, this allows you to write rules in your favorite JVM language, including Scala and Kotlin! SKYROCKETING TO THE CLOUD: THE DROOLS QUARKUS EXTENSION  We are also working on introducing a new Drools-focused Quarkus extension; this extension is dedicated to enable the use of the Drools core engine as part of a Quarkus application. Please notice this new Drools Quarkus extension is not a replacement for . For more comprehensive capabilities in cloud-native scenarios which are not part of this new extension, such as automatic REST endpoint generation, we invite developers to keep using and explore Kogito. Because of its experimental nature, it is currently not available on the main Quarkus channels, but you can find a and a in the Drools repository at those links. NOT JUST RULES: A MODULAR ENGINE FOR AI Up to version 7 Drools was a monolithic rule engine with all its features implemented (and tightly interconnected) in one single core module. In Drools 8 the engine has been heavily reworked to make it more modular, isolating the non-core features, like the Truth Maintenance System (TMS), the protobuf based serialization and the experimental support for traits and different belief systems, in separated modules that the users can optionally bring into their projects only when specifically required.  But this modularization effort didn’t stop there. The main and ambitious goal is to make Drools a container where many other AI related technologies, like DMN and PMML, can coexist and cooperate. With this goal in mind it has developed an internal framework, with codename Efesto, allowing to plug different engines and to let them leverage each other in a loosely coupled way.  CODENAME EFESTO: AN INTERNAL COMPILE-TIME AND RUN-TIME COORDINATION FRAMEWORK Over the years a common pattern emerged in our engines. They all have a phase where, very broadly, a source model is translated to an executable unit (compilation); and they all have a phase where that executable unit is used to make actual evaluation (runtime). Besides, some of them may need to invoke others for full evaluation, e.g. some decision models invoke prediction’ ones,  and some prediction models use the rule engine. Efesto is the codename we gave the internal framework we designed to provide a common, stable, unified API to support such phases, and allow to chain the processing of different models, either at compile-time or at run-time. The benefit is improved code reusability and overall simplification of such processing phases. We plan to use this new API both for integrating engines with each other (when a source model needs to refer to a different source model) and to easily extend usage of current implementations in different environments. More details about the overall design can be found in , while a deeper technical explanation has been provided in . RULE UNITS: A NEW MODULAR PROGRAMMING MODEL FOR RULES The definition of large rule bases has always made evident the need of slicing it in logically independent subparts. In the past Drools fulfilled this requirement in a few different ways: it is possible to split the rules in multiple KieBase and make a KieBase to include others, rules in the same KieBase can be further organized into agenda-groups and also the working memory can be partitioned unisg entry-points. A rule unit is a unified, top-down module, describing a set of rules; the unit describes also the shape of the data that such rules are able to manipulate, through the introduction of the "data source" abstraction that partitions the working memory into the typed equivalent of an entry point. In other words, a rule unit encapsulates the unit of execution for rules and the data against which the rules will be matched. More information about how rule units and data sources are available .  The new Java-based DSL builds upon the rule unit concept, but you can still define rule units using good ol’ DRL! EXPERIMENTAL FEATURES AND WHAT’S NEXT The Drools team keeps experimenting with new features that could bring new capabilities to the engine or improve the overall user experience. At the moment the efforts to bring forward these experiments are concentrated in 2 main areas. IMPACT ANALYSIS The feature analyzes the relationships between the rules generating an oriented graph that shows which rules can be impacted by the firing of another rule. In this way, when it is necessary to analyze the impact of a change to a specific rule, it becomes easy to visually determine which part of a rules set will be interested by this change. DRL CODE EDITOR A DRL code editor based on Language Server Protocol (LSP) is currently under development. LSP could be applied to various IDEs and editors like VS Code and Neovim via extensions, plugins, or configuration. Not yet released at the moment, but stay tuned! FROM DROOLS 7 TO 8 Despite all the new features introduced during the development of Drools 8, the old API based on the old KieContainer/Base/Session is still fully working and supported, so if you’re not comfortable in migrating immediately your existing codebase relying on the Drools 7 API, this won’t prevent you to upgrade to Drools 8. Nevertheless we strongly suggest you to also give a look at the new rule unit based programming model and evaluate if it could be a better fit for your needs. If you are still in doubt, try the Java-based DSL and let us know! These can be very helpful to support your migration plans from Drools 7 to Drools 8, including directions on how to . JDK 11 is now the minimum requirement to run Drools, while at least maven 3.8.6 is necessary to build Drools from sources or use the maven integration capabilities provided by the kie-ci module. More information on this, including a minimal list of the features that have been deprecated with Drools 8 or retired, is available in the .  A NOTE ON VERSION NUMBERING It may be surprising that we deployed the first stable release of Drools 8 Final with version Drools 8.29.0.Final. We did so for the twofold reason of continuing with the numbering of the minor version that we started while Drools 8 was still in beta and avoiding confusion with other downstream projects of the kie group like Kogito and OptaPlanner that are aligned with the same minor version. REFERENCES You can find the . The post appeared first on .</content><dc:creator>Mario Fusco</dc:creator></entry><entry><title>How to fix issues caused by reusing Go error variables</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/10/17/fix-issues-reusing-go-error-variables" /><author><name>Massimiliano Ziccardi</name></author><id>08db59ca-2473-4722-9329-8eb2c8ccac4e</id><updated>2022-10-17T07:00:00Z</updated><published>2022-10-17T07:00:00Z</published><summary type="html">&lt;p&gt;This article discusses issues caused by reusing error variables in &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt; code. I will begin by illustrating the problem with a fictional company called ACME Corporation and its software engineer Wile E. Then I will present trivial examples to reveal the problem and how to avoid it.&lt;/p&gt; &lt;h2&gt;The story of a reused Go error variable&lt;/h2&gt; &lt;p&gt;A user opened a new issue to ACME Corporation because he noticed that users could enter any string into the phone number on the contact form. Wile E. is in charge of implementing a fix.&lt;/p&gt; &lt;p&gt;Wile E. examines the code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;func StoreAllContacts() { for _, c := range contactsToStore { err := legacy.StoreContact(&amp;c) if err != nil { fmt.Printf("Error storing %v: %v\n", c, err) } else { fmt.Printf("Contact NAME: %s, SURNAME: %s, PHONE: %s stored successfully!\n", c.Name, c.Surname, c.PhoneNumber) } } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When he runs the code, he sees the following output:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;Contact NAME: John, SURNAME: Doe, PHONE: (012) 3456789 stored successfully! Contact NAME: Jane, SURNAME: Doe, PHONE: (089) 3456789 stored successfully! Contact NAME: Tom, SURNAME: Doe, PHONE: (083) 1234567 stored successfully!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Wile thinks, "OK. Fixing the code should be easy. I will add the check for the phone number format just before calling the &lt;code&gt;StoreContact&lt;/code&gt; function."&lt;/p&gt; &lt;p&gt;Then, Wile produces the following code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;const phoneNumberRe = `\(\d{3}\) \d{7}` func validatePhoneNumber(phoneNumber string) (bool, error) { re, err := regexp.Compile(phoneNumberRe) if err != nil { return false, err } return re.MatchString(phoneNumber), nil } func StoreAllContacts() { for _, c := range contactsToStore { valid, err := validatePhoneNumber(c.PhoneNumber) if err != nil { fmt.Printf("Invalid phone number RE ('%s'). Validation ignored.\n", phoneNumberRe) } if !valid { fmt.Printf("Invalid phone number '%s' for '%s %s'", c.PhoneNumber, c.Name, c.Surname) } err = legacy.StoreContact(&amp;c) if err != nil { fmt.Printf("Error storing %v: %v\n", c, err) } } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Wile runs the code, but the output is not what he expected:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;Error storing {John Doe (012) 3456789}: &lt;nil&gt; Error storing {Jane Doe (089) 3456789}: &lt;nil&gt; Error storing {Tom Doe (083) 1234567}: &lt;nil&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Wile wonders what is happening: "The program enters the &lt;code&gt;if err != nil&lt;/code&gt; branch, but then the output of the &lt;code&gt;fmt.Printf &lt;/code&gt;shows that &lt;code&gt;err&lt;/code&gt; is &lt;code&gt;nil&lt;/code&gt;..."&lt;/p&gt; &lt;p&gt;What makes Wile even more confused is that he didn't change anything inside &lt;code&gt;StoreContact.&lt;/code&gt; So he wonders why the behavior changed.&lt;/p&gt; &lt;p&gt;After some debugging, Wile decides to look at the &lt;code&gt;StoreContact&lt;/code&gt; method:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;func StoreContact(c *Contact) *ContactError { if err := c.validate(); err != nil { return err } // ... store the contact somewhere return nil }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Wile: "Oh! There it is!"&lt;/p&gt; &lt;p&gt;Wile gets permission to fix the function as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;func StoreContact(c *Contact) error { if err := c.validate(); err != nil { return err } // ... store the contact somewhere return nil } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When he reruns the code, it finally works as expected. But how did he arrive at this fix?&lt;/p&gt; &lt;h2&gt;The fix explained: The hidden dynamic type&lt;/h2&gt; &lt;p&gt;To understand the issue, let's look at the following code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;package main import "fmt" type MyTestError struct { } func (MyTestError) Error() string { return "Test Error String" } func main() { var err error // (1) fmt.Println("Is err nil?", err == nil) var mytesterror *MyTestError // (2) fmt.Println("Is mytesterror nil?", mytesterror == nil) err = mytesterror // (3) fmt.Println("Is err nil?", err == nil) } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here are descriptions of the commented-out numbers:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;We create a variable of type &lt;code&gt;error&lt;/code&gt;. We didn't assign any value to it, so it will be &lt;code&gt;nil&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;We create a variable of type &lt;code&gt;*MyTestError&lt;/code&gt;. As before, we didn't give any value to it, so it will be &lt;code&gt;nil&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;We assign &lt;code&gt;mytesterror&lt;/code&gt; (nil) to &lt;code&gt;err&lt;/code&gt;. Since &lt;code&gt;mytesterror&lt;/code&gt; is nil, should be nil too.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Let's try to run the code (you can run it at the &lt;a href="https://go.dev/play/p/0kiJPD8NAEe"&gt;Go Playground&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;Is err nil? true Is mytesterror nil? true Is err nil? false&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The reason for this unexpected behavior is that the &lt;code&gt;interface&lt;/code&gt; type in Go is composed of a &lt;code&gt;value&lt;/code&gt; and a &lt;code&gt;dynamic type&lt;/code&gt;. The interface is considered &lt;code&gt;nil&lt;/code&gt; only when the value and the type are &lt;code&gt;nil&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;With that in mind, let's look again at the code:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;We create a variable of type &lt;code&gt;error&lt;/code&gt;. We didn't assign any value to it, so it will be &lt;code&gt;nil&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;We create a variable of type &lt;code&gt;*MyTestError&lt;/code&gt;. As before, we didn't give any value to it, so it will be &lt;code&gt;nil&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;We assign &lt;code&gt;mytesterror&lt;/code&gt; (that is, &lt;code&gt;nil&lt;/code&gt;) to &lt;code&gt;err&lt;/code&gt;. After this assignment, the value of the &lt;code&gt;err&lt;/code&gt; interface is &lt;code&gt;nil&lt;/code&gt;, but the type is &lt;code&gt;*MyTestError&lt;/code&gt;. As explained, an interface with a non-nil type is not nil even if the value is &lt;code&gt;nil&lt;/code&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;To make the issue more evident, let's run the following code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;package main import "fmt" type MyTestError struct { } func (MyTestError) Error() string { return "Test Error String" } func main() { var err error // (1) fmt.Println("Type of err:", reflect.TypeOf(err)) fmt.Println("Is err nil?", err == nil) var mytesterror *MyTestError // (2) fmt.Println("Type of mytesterror:", reflect.TypeOf(mytesterror)) fmt.Println("Is mytesterror nil?", mytesterror == nil) err = mytesterror // (3) fmt.Println("Type of err:", reflect.TypeOf(err)) fmt.Println("Is err nil?", err == nil) }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output is:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;Type of err: &lt;nil&gt; Is err nil? true Type of mytesterror: *main.MyTestError Is mytesterror nil? true Type of err: *main.MyTestError Is err nil? false&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The assignment changed the type of &lt;code&gt;err&lt;/code&gt; from &lt;code&gt;&lt;nil&gt;&lt;/code&gt; to &lt;code&gt;*main.MyTestError.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The same happens with code like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;package main import ( "fmt" "reflect" ) type MyTestError struct { } func (MyTestError) Error() string { return "Test Error String" } func MyTestFunc() error { var err *MyTestError return err } func main() { err := MyTestFunc() if err != nil { fmt.Println("An error has occurred: ", err) fmt.Println("Type of err: ", reflect.TypeOf(err)) return } fmt.Println("Success!") }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you try to run it in &lt;a href="https://go.dev/play/p/sZm1T9dAtWd"&gt;Go Playground&lt;/a&gt;, you will get the following output:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;An error has occurred: &lt;nil&gt; Type of err: *main.MyTestError&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The reason why this happens should now be apparent by now: to return the error, Go copied the value of &lt;code&gt;err&lt;/code&gt; (of type &lt;code&gt;*MyTestError&lt;/code&gt;) to a variable of type &lt;code&gt;error&lt;/code&gt;: exactly as we did in the previous example.&lt;br /&gt; We can quickly fix the code returning a naked nil:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;package main import ( "fmt" "reflect" ) type MyTestError struct { } func (MyTestError) Error() string { return "Test Error String" } func MyTestFunc() error { var err *MyTestError err = err // let's do something with the variable to avoid compiling error return nil } func main() { err := MyTestFunc() if err != nil { fmt.Println("An error has occurred: ", err) fmt.Println("Type of err: ", reflect.TypeOf(err)) return } fmt.Println("Success!") }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Running the code in the &lt;a href="https://go.dev/play/p/pHjClQDDhWw"&gt;Go Playground &lt;/a&gt;we now get the following output:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;Success!&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Three cases to keep in mind&lt;/h2&gt; &lt;p&gt;Reusing error variables and returning custom error structures is a widespread practice, and there isn't anything wrong with that. However, you should keep the form of the returned error in mind when checking it for a &lt;code&gt;nil&lt;/code&gt; value:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;If you control the source code of the called function, ensure it returns an &lt;code&gt;error&lt;/code&gt; interface and always returns a naked &lt;code&gt;nil&lt;/code&gt; value.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;If the function you call returns a concrete object, use a dedicated variable to store that function error.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;In all the other cases, if you see weird behaviour where Go says that a &lt;code&gt;nil&lt;/code&gt; variable is &lt;code&gt;not nil&lt;/code&gt;, you can always use the &lt;code&gt;reflect&lt;/code&gt; package to check the value specifically:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;reflect.ValueOf(err).IsNil()&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;I hope this has expanded your knowledge of the Go language. Please leave a comment below if you have questions about this article. As always, we welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/10/17/fix-issues-reusing-go-error-variables" title="How to fix issues caused by reusing Go error variables"&gt;How to fix issues caused by reusing Go error variables&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Massimiliano Ziccardi</dc:creator><dc:date>2022-10-17T07:00:00Z</dc:date></entry><entry><title>JBoss Tools 4.25.0.Final for Eclipse 2022-09</title><link rel="alternate" type="text/html" href="https://tools.jboss.org/blog/4.25.0.final.html" /><category term="release" /><category term="jbosstools" /><category term="devstudio" /><category term="jbosscentral" /><category term="codereadystudio" /><author><name>jeffmaury</name></author><id>https://tools.jboss.org/blog/4.25.0.final.html</id><updated>2022-10-19T07:19:04Z</updated><published>2022-10-17T00:00:00Z</published><content type="html">&lt;div&gt;&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Happy to announce 4.25.0.Final build for Eclipse 2022-09.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Downloads available at &lt;a href="https://tools.jboss.org/downloads/jbosstools/2022-09/4.25.0.Final.html"&gt;JBoss Tools 4.25.0.Final&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="what-is-new"&gt;&lt;a class="anchor" href="#what-is-new"&gt;&lt;/a&gt;What is New?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Full info is at &lt;a href="https://tools.jboss.org/documentation/whatsnew/jbosstools/4.25.0.Final.html"&gt;this page&lt;/a&gt;. Some highlights are below.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="general"&gt;&lt;a class="anchor" href="#general"&gt;&lt;/a&gt;General&lt;/h3&gt; &lt;div class="sect3"&gt; &lt;h4 id="java-17-requirement"&gt;&lt;a class="anchor" href="#java-17-requirement"&gt;&lt;/a&gt;Java 17 requirement&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Java 17 is now a minimum requirement to &lt;strong&gt;run&lt;/strong&gt; JBoss Tools. JBoss Tools continues to support running servers and applications with older Java versions.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="quarkus-tools"&gt;&lt;a class="anchor" href="#quarkus-tools"&gt;&lt;/a&gt;Quarkus Tools&lt;/h3&gt; &lt;div class="sect3"&gt; &lt;h4 id="improvement-to-the-new-quarkus-project-wizard"&gt;&lt;a class="anchor" href="#improvement-to-the-new-quarkus-project-wizard"&gt;&lt;/a&gt;Improvement to the new Quarkus project wizard&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The Quarkus extension ecosystem is composed of extensions that are part of the platform and the others. The Quarkus project wizard has been extended to allow exclusion of extensions that are not part of the platform.&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;img src="https://tools.jboss.org/documentation/whatsnew/quarkus/images/quarkus46.gif" alt="quarkus46" /&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="hibernate-tools"&gt;&lt;a class="anchor" href="#hibernate-tools"&gt;&lt;/a&gt;Hibernate Tools&lt;/h3&gt; &lt;div class="sect3"&gt; &lt;h4 id="new-runtime-provider"&gt;&lt;a class="anchor" href="#new-runtime-provider"&gt;&lt;/a&gt;New Runtime Provider&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A new runtime provider has been added for Hibernate 6.1. It incorporates Hibernate Core version 6.1.3.Final and Hibernate Tools version 6.1.3.Final.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect3"&gt; &lt;h4 id="runtime-provider-updates"&gt;&lt;a class="anchor" href="#runtime-provider-updates"&gt;&lt;/a&gt;Runtime Provider Updates&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The Hibernate 5.6 runtime provider now incorporates Hibernate Core version 5.6.12.Final and Hibernate Tools version 5.6.12.Final.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="and-more"&gt;&lt;a class="anchor" href="#and-more"&gt;&lt;/a&gt;And more…​&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You can find more noteworthy updates in on &lt;a href="https://tools.jboss.org/documentation/whatsnew/jbosstools/4.24.0.AM1.html"&gt;this page&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Jeff Maury&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;</content><summary>Happy to announce 4.25.0.Final build for Eclipse 2022-09. Downloads available at JBoss Tools 4.25.0.Final. What is New? Full info is at this page. Some highlights are below. General Java 17 requirement Java 17 is now a minimum requirement to run JBoss Tools. JBoss Tools continues to support running servers and applications with older Java versions. Quarkus Tools Improvement to the new Quarkus project wizard The Quarkus extension ecosystem is composed of extensions that are part of the platform and the others. The Quarkus project wizard has been extended to allow exclusion of extensions that are not part of the platform. Hibernate Tools New Runtime Provider A new runtime provider has been added for Hibernate...</summary><dc:creator>jeffmaury</dc:creator><dc:date>2022-10-17T00:00:00Z</dc:date></entry></feed>
